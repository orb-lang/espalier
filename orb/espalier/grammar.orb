* Grammar Module


  The grammar module returns one function, which generates a grammar.


** Introduction

This module is in a very real sense a *duet*.

It is an adaptation, refinement, extension, of Phillipe Janda's work,
[[luaepnf][http://siffiejoe.github.io/lua-luaepnf/]].

While =helm= was built from a repl provided by Tim Caswell in =luv=, that is a
case of taking a sketch and painting a picture.

Many difficult aspects of this algorithm are found directly in the source
material upon which this is based.

Don Phillipe has my thanks, and my fervent hope that he enjoys what follows.


** Parameters of the Grammar Function

This function takes two parameters, namely:

  - grammar_template :  A function with one parameter, which must be =_ENV=.
  - metas :  A map with keys of string and values of Node subclass
             constructors.

Both of these are reasonably complex.


*** grammar_template

  The internal function =define= creates a custom environment variable, neatly
sidestepping Lua's pedantic insistance on prepending =local= to all values of
significance.

Thus equipped, it constructs a full grammar, which will return a table of type
Node.

If you stick to =lpeg= patterns, as you should, all array values will be of
Node.  Captures will interpolate various other sorts of Lua values, which will
induce halting in some places and silently corrupt execution in others.

You can use captures in your rules, if it's helpful, as with named groups,
just toss them away at the end of the rule like so:

#!lua-example
divisible_by_three = Cmt( C(R"09"^1),
   function(s, i, val)
      if tonumber(val) % 3 == 0
         return true
      else
         return false
      end
   end ) / 0
#/lua-example

Giving a rule which matches an integer evenly divisible by three.

The [[elpatt module][@:espalier/elpatt]] is intended to provide those
patterns which are allowed in Grammars, while expanding the scope of some
favorites to properly respect utf-8 and otherwise behave.

Also included are two functions:

  -  START :  A string which must be the same as the starting rule.  Required.
  -  SUPPRESS :  Either a string or an array of strings. These rules will be
                 removed from the resulting AST, while keeping all leaf nodes,
                 if any.  Optional.

The use of ALL-CAPS was Phillipe Janda's convention, I agree that it reads
well in this singular instance.


*** metas

  By default a node will inherit from the Node class.  If you want custom
behavior, you must pass in a table of metatable constructors.

That's a fairly specific beast.  Any rule defined above will have an =id=
corresonding to the name of the rule.  Unless =SUPPRESS=ed, this will become
a Node.

The =metas= parameter will have keys which correspond to the =.id= field of a
given rule.  The value is either a table, or a function.

If a table, this must inherit from Node via =:inherit(id)=.  These are simply
assigned as the metatable for the node in question.

If a function, that function will receive =(node, str)=; =node= will have =id=,
=first=, =last=, and =str= parameters.  The =offset= parameter is usually 0,
and represents what must be added to =first= and =last= to get an accurate
value, in the event that the Grammar is parsing a substring of a larger string.

Whatever the function returns is assigned a parent and inserted as a Node, so
it should really be a table with a Node-descended metatable, or all manner of
methods will fail to function.

One useful function, which provides the motive for this construction, is a
subgrammar. The Grammar constructor returns a function, which has optional
parameters for a =first= and =last=.  If provided, it will parse only over
that slice of the string, with the indices correctly adjusted.  Using this
produces the  =offset= parameter which we saw above.

So a subgrammar is called thus: =subgrammar(node, node:bounds())=, and the
return value will be inserted as a sub-node.  Which is hopefully well-formed
with respect to the rest of the tree, and will be, if a subgrammar is used
carefully.

The generic subgrammar isn't always adequate, but serves to provide a template
for the sort of function which must be provided.

The result of all these machinations is that we're able to define a rule twice:
once with a loose rule, adequate to uniquely identify the rule and its
boundaries, and a second which can resolve in more detail, and freed from the
requirement to cooperate with the rest of the containing Grammar.

Subgrammars may themselves receive a =metas= parameter, so this procedure can
be repeated as often as necessary.

If a metatable of the given =.id= is not provided, the metatable at =metas[1]=
is used instead.  If no default is provided, this defaults to Node, just as if
the =metas= parameter is =nil=.

This lets us define a =metas= parameter thus:

#!lua-example
local metas = { Default,
                a = A,
                b = Bfn, } -- etc
#/lua-example

Which combines nicely with using =:inherit(id)= to make a base class, which
other Nodes can specify through further inheritance.


** Implementation

All of =espalier= is in principle compatible with the entire '5' series of
Luas.  Any failure to execute through at least 5.4 is considered a bug.


*** imports

We follow a strict coding style for admitting dependencies into the module,
localizing everything as an upvalue before using it.


**** requires


***** status

#!lua
local s = require "status:status" ()
s.verbose = false
s.angry   = false
#/lua


**** requires, contd.

#!lua
local L = require "lpeg"
local compact = assert(require "core/table" . compact)
local Node = require "espalier/node"
#/lua

I like the dedication shown in this style of import.

It's the kind of thing I'd like to automate.

We don't normally assert single variables in the global namespace, but this
was included in the original epnf, and I saw no reason to remove it.


**** asserts

#!lua
local assert = assert
local string = assert(string)
local sub = assert(string.sub)
local remove = assert(table.remove)
local VER = sub(assert(_VERSION), -4)
local _G = assert(_G)
local error = assert(error)
local pairs = assert(pairs)
local next = assert(next)
local type = assert(type)
local tostring = assert(tostring)
local setmeta = assert(setmetatable)
if VER == " 5.1" then
   local setfenv = assert(setfenv)
   local getfenv = assert(getfenv)
end
#/lua


*** make_ast_node

This takes a lot of parameters and does a lot of things.

#!lua
local function make_ast_node(id, first, t, last, str, metas, offset)
#/lua

- Parameters:
  - id      :  'string' naming the Node
  - first   :  'number' of the first byte recognized from =str=
  - t       :  'table' capture of grammatical information
  - last    :  'number' of the last byte recognized from =str=
  - str     :  'string' being parsed
  - metas   :  'table' of Node-inherited metatables (complex)
  - offset  :  'number' of optional offset.  This would be provided if
               e.g. byte 1 of =str= is actually byte 255 of a larger
               =str=.  Normally 0.

=first=, =last= and =offset= follow Wirth indexing conventions.

Because of course they do.


**** Set up values and metatables

  We accept two types of value for a metatable. A table must be derived from
the Node class, while a function must return an appropriately-shaped table,
given the capture and offset.

This can be used to process captures which aren't strings, perform validation,
or run another grammar and return an entire AST, but currently cannot fail to
return a Node of some sort.

#!lua
   t.first = first + offset
   t.last  = last + offset - 1
   t.str   = str
   if metas[id] then
      local meta = metas[id]
      if type(meta) == "function" then
        t.id = id
        t = meta(t, offset)
      else
        t = setmeta(t, meta)
      end
      assert(t.id, "no id on Node")
   else
      t.id = id
      setmeta(t, metas[1])
   end

   if not t.parent then
      t.parent = t
   end
#/lua


**** Drop non-Nodes

  We discourage you to use captures inside grammars, and if you do, it's
better to discard them.

But just in case, we iterate and drop anything which isn't a Node.

It's actually possible that everything below here, up to the return, isn't
necessary.  I'll leave it in for now; if it does guard against problems, they
would be difficult ones to debug.

#!lua
   local top, touched = #t, false
   for i = 1, top do
      local cap = t[i]
      if type(cap) ~= "table" or not cap.isNode then
         touched = true
         t[i] = nil
      else
         cap.parent = t
      end
   end
   if touched then
      compact(t, top)
   end
#/lua


**** post-conditions and return

These guard against certain simple mistakes which could arise from the use of
subgrammars.

#!lua
   -- post conditions
   assert(t.isNode, "failed isNode: " .. id)
   assert(t.str)
   assert(t.parent, "no parent on " .. t.id)
   return t
end
#/lua


** define(func, g, e)

This is [[Phillipe Janda][http://siffiejoe.github.io/lua-luaepnf/]]'s
algorithm, with my adaptations.

=func= is the grammar definition function, pieces of which we've provided.
We'll see how the rest is put together presently.

=e=, either is or becomes =_ENV=.

This is not needed in LuaJIT, while for Lua 5.2 and above, it is.

=g= is, or becomes, a =Grammar=.


**** localizations

We localize the patterns we use.

#!lua
local Cp = L.Cp
local Cc = L.Cc
local Ct = L.Ct
local arg1_str = L.Carg(1)
local arg2_metas = L.Carg(2)
local arg3_offset = L.Carg(3)
#/lua

Setup an environment where you can easily define lpeg grammars with lots of
syntax sugar, compatible with the 5 series of Luas:

#!lua
local function define(func, g, e)
   g = g or {}
   if e == nil then
      e = VER == " 5.1" and getfenv(func) or _G
   end
   local suppressed = {}
   local env = {}
   local env_index = {
      START = function(name) g[1] = name end,
      SUPPRESS = function(...)
         suppressed = {}
         for i = 1, select('#', ...) do
            suppressed[select(i, ... )] = true
         end
      end,
      V = L.V,
      P = L.P }

    setmeta(env_index, { __index = e })
    setmeta(env, {
       __index = env_index,
       __newindex = function( _, name, val )
          if suppressed[ name ] then
             g[ name ] = val
          else
             g[ name ] = Cc(name)
                       * Cp()
                       * Ct(val)
                       * Cp()
                       * arg1_str
                       * arg2_metas
                       * arg3_offset / make_ast_node
          end
       end })

   -- call passed function with custom environment (5.1- and 5.2-style)
   if VER == " 5.1" then
      setfenv(func, env )
   end
   func( env )
   assert( g[ 1 ] and g[ g[ 1 ] ], "no start rule defined" )
   return g
end
#/lua


*** refineMetas(metas)

Takes metatables, distributing defaults and denormalizations.

#!lua
local function refineMetas(metas)
  for id, meta in pairs(metas) do
    if id ~= 1 then
      if type(meta) == "table" then
        -- #todo is this actually necessary now?
        -- if all Node children are created with Node:inherit then
        -- it isn't.
        if not meta["__tostring"] then
          meta["__tostring"] = Node.toString
        end
        if not meta.id then
          meta.id = id
        end
      end
    end
  end
  if not metas[1] then
     metas[1] = Node
  end
  return metas
end
#/lua


** new

Given a grammar_template function and a set of metatables,
yield a parsing function and the grammar as an =lpeg= pattern.


**** _fromString(g_str), _toFunction(maybe_grammar)

Currently this is expecting pure Lua code; the structure of the module is
such that we can't call the PEG grammar from =grammar.orb= due to the
circular dependency thereby created.

#Todo we actually could try and make a Peg here, by just-in-time =require= ing
the module, since it would happen at run time, not load time.  This might not
be worthwhile, but it's worth thinking about at least.

This implies wrapping some porcelain around everything so that we can at least
try to build the declarative form first.

#!lua
local function _fromString(g_str)
   local maybe_lua, err = loadstring(g_str)
   if maybe_lua then
      return maybe_lua()
   else
      s : halt ("cannot make function:\n" .. err)
   end
end

local function _toFunction(maybe_grammar)
   if type(maybe_grammar) == "string" then
      return _fromString(maybe_grammar)
   elseif type(maybe_grammar) == "table" then
      -- we may as well cast it to string, since it might be
      -- and sometimes is a Phrase class
      return _fromString(tostring(maybe_grammar))
   end
end

local P = assert(L.P)

local function new(grammar_template, metas, pre, post)
   if type(grammar_template) ~= "function" then
      -- see if we can coerce it
      grammar_template = _toFunction(grammar_template)
   end

   local metas = metas or {}
   metas = refineMetas(metas)
   local grammar = define(grammar_template, nil, metas)

   local function parse(str, start, finish)
      local sub_str, begin = str, 1
      local offset = start and start - 1 or 0
      if start and finish then
         sub_str = sub(str, start, finish)
      end
      if start and not finish then
         begin = start
         offset = 0
      end
      if pre then
         str = pre(str)
         assert(type(str) == "string")
      end

      local match = L.match(grammar, sub_str, begin, str, metas, offset)
      if match == nil then
         return nil
      elseif type(match) == 'number' then
         return sub(sub_str, 1, match)
      end
      if post then
        match = post(match)
      end
      match.complete = match.last == #sub_str + offset
      return match
   end

   return parse, grammar
end
#/lua

#!lua
return new
#/lua


* Generalizing

Espalier has been a rousing success thus far.

I continue to use the declarative PEG specification to expand Orb, with no
regrets.

The most important consequence of using a declarative specification has yet
to be realized, however.  Nor can we get there with the codebase written as it
is.

Yet we're very close!  Currently, a PEG generates Lua code, which is then
passed to =define=, which produces exactly one sort of artifact: a parser
which transforms a string into an Abstract (or Concrete) Syntax Tree.

However, there is no hard-coded reason why this needs to be the only sort of
code which a Grammar can generate.  The Lua code which is created from a PEG
specification doesn't invoke Lpeg directly, although it appears to: it calls a
number of functions named =P=, =R=, and so on, doing most of the heavy lifting
through predefined operator overloading.

The interesting part is that the operator overloading is simply parser
combinators: and we can define similar combinators for other purposes using
the same conventions, such that the generated functions can be purposed
towards, for a motivating example, confirming that the shape of a given Node
could have been produced from the Grammar in question.

Or, we could change the target to Hammer, and use some clever user types to
get the same return values as Lpeg, while using packrat parsing.

Even if we stick with the Lpeg code exactly as generated, passing it to the
sole =define= function we currently have is overly specified.  Instead of
constructing an in-memory AST, we could perform syntax highlighting, as HTML,
as terminal colors, however LSPs do it: there are a lot of options.

The case I'm interested in tackling directly is turning [[LON data]
[@br.lon:lon]] directly into Lua tables, without bothering to pass through an
abstract syntax tree.  This would only be attractive if we could generate it
from the same specification which creates a Node, otherwise, since we need (at
least want) the Node version, it would be more expedient to generate a table
from that.

To make this usefully general, instead of metatables we would pass in *action
rules*, such that recognition of each category would call a function with a
useful signature.  This is most probably the same parameters we pass to
=make_ast_node=, along with an additional table parameter, since this can be
made to suffice for all accumulation.

We can most likely just return this table when the call to the parser is
completed, since we would be remiss to not allow for =pre= and =post=
functions, and could thereby extract, concatenate, or otherwise manipulate the
resulting data.
