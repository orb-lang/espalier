* Mem


The tools of higher combination.

#!lua
local Clade, Node = use ("cluster:clade", "espalier:peg/node")
#/lua


**** Clade Extension

#!lua
local function postindex(tab, field)
   tab[field].tag = field
   return tab[field]
end

local contract = {postindex = postindex, seed_fn = true}

local MemClade = Clade(Node, contract):extend(contract)
local Mem = MemClade.tape
local Basis = Mem[1]
local Mem_M = MemClade.meta[1]

Basis.v = 1
#/lua

This is where we crack our knuckles and start porting.


**** imports

#!lua
local core = use "qor:core"
local table = core.table
local Set = core.set
local Deque = use "deque:deque"
local insert, remove, concat = assert(table.insert),
                               assert(table.remove),
                               assert(table.concat)
local s = use "status:status" ()
s.verbose = false
#/lua


***** normalize

  Causes any =-= in a name to become =_=, allowing us to treat them as
interchangeable.  This is my crank compromise between ordinary string
matching and creations such as Nim's symbol equivalence and unified call
syntax.

#!lua
local gsub = assert(string.gsub)

local function normalize(str)
   return gsub(str, "%-", "%_")
end
#/lua


*** Qualia

Aka Traits,

#!lua
local Q = {}
#/lua


**** Q.nofail

Any rule type which matches the empty string.

Note there is one type of repeat rule =name%0..5= which is also optional but
we must detect this (we do not as yet).

#!lua
Q.nofail = Set {'zero_plus', 'optional'}
#/lua


**** Q.predicate

A match which will never advance the cursor, but which can succeed or fail.

#!lua
Q.predicate = Set {'and', 'not'}
#/lua


**** Q.failsucceeds

A rule which succeeds by failing, aka =not=.

There is a reason to have a category of one, which is that this condition
propagates beyond its definition during rule constraint.

#!lua
Q.failsucceeds = Set {'not'}
#/lua


**** Q.nullable

A rule which can match without advancing the cursor.

#!lua
Q.nullable = Q.predicate + Q.nofail
#/lua


**** Q.compound

#!lua
Q.compound = Set {'cat', 'alt'}
#/lua


**** Q.terminal

Rules which produce a definite amount of cursor movement in and of themselves.

#!lua
Q.terminal = Set {'literal', 'set', 'range', 'number'}
#/lua


**** Q.unbounded

Rules which can consume arbitrary amounts of input inherently.

This doesn't account for recursive analysis: = A <- "a" A= type grammars can
be identified, and will be, but not by class.

#!lua
Q.unbounded = Set { 'zero_plus', 'one_plus' }
#/lua

Which we can also represent inside-out as traits pertaining to a class:

#!lua
local Prop = {}
for trait, classSet in pairs(Q) do
   for class in pairs(classSet) do
      Prop[class] = Prop[class] or {}
      insert(Prop[class], trait)
   end
end
for class, array in pairs(Prop) do
   Prop[class] = Set(array)
end
#/lua


** Mem Basis


***** mem:parentRule()

Returns the parent rule of the part.

#!lua
function Basis.parentRule(mem)
   if mem.tag == 'rule' then return nil, 'this is a rule' end
   if mem.tag == 'grammar' then return nil, 'this is a grammar' end
   local parent = mem.parent
   repeat
      if parent.tag == 'rule' then
         return parent
      else
         parent = parent.parent
      end
   until parent:isRoot()

   return nil, 'mistakes were made (new tree structure?)'
end
#/lua


***** mem:nameOfRule() #deprecated :withinRule()

Returns the name of the enclosing rule.

#!lua
function Basis.nameOfRule(mem)
   local rule, why = mem:parentRule()
   if not rule then
      return nil, why
   end
   return rule.token
end

function Basis.withinRule(mem)
   s:chat "use .nameOfRule"
   return mem:nameOfRule()
end
#/lua


**** mem:nameOf()



#!lua
function Basis.nameOf(mem)
   return mem.name or mem.tag
end
#/lua


***** Synthesis

Here we decorate particular Nodes with useful representations and
contextual information.

#!lua
local SpecialSnowflake = Set {'set', 'range', 'name',
                               'number', 'literal', 'rule_name'}

local function extraSpecial(node)
   local c = node.tag
   if c == 'range' then
      node.from_char, node.to_char = node[1]:span(), node[2]:span()
   elseif c == 'set' then
      node.value = node:span()
   elseif c == 'name' or c == 'rule_name' then
      node.token = normalize(node:span())
   else
      node.token = node:span()
   end
end
#/lua

#!lua
local analyzeElement;

local Hoist = Set {'element', 'alt', 'cat'}

local function synthesize(node)
   for _, twig in ipairs(node) do
      if Hoist[twig.tag] then
         if twig:hoist() then
            twig = assert(twig[1])
         end
      end

      if SpecialSnowflake[node.tag] then
         extraSpecial(twig)
      end
      -- elements
      if twig.tag == 'element' then
         analyzeElement(twig)
      end
      if node.tag == 'rule' then
         node.token = normalize(node :take 'rule_name' :span())
      end
      synthesize(twig)
   end
   return node
end
#/lua


**** analyzeElement(elem)

The parser change puts all the modifiers on the element, which is useful.

Since we hoist a few redundant classes, any element which remains is modified
in some fashion, what we do here is promote that information onto the
element, and copy it to the component part, such that we no longer need to
consider those synth nodes.

We then dispose of the surroundings, except for back references.

#!lua
local Prefix = Set {'and', 'not', 'to_match'}
local Suffix = Set {'zero_plus', 'one_plus', 'optional', 'repeat'}
local Backref = Set {'backref'}

local Surrounding = Prefix + Suffix + Backref
#/lua

#!lua
function analyzeElement(elem)
   local prefixed, backrefed  = Prefix[elem[1].tag],
                                Backref[elem[#elem].tag]
   local suffixed;
   if backrefed then
      suffixed = Suffix[elem[#elem-1].tag]
   else
      suffixed = Suffix[elem[#elem].tag]
   end
   local modifier = { prefix = false,
                      suffix = false,
                      backref = false, }

   local part

   if prefixed then
      modifier.prefix = elem[1]
      part = elem[2]
   else
      part = elem[1]
   end

   if backrefed and suffixed then
      modifier.backref = elem[#elem]
      modifier.suffix  = elem[#elem - 1]
   elseif suffixed then
      modifier.suffix = elem[#elem]
   elseif backrefed then
      modifier.backref = elem[#elem]
   end
   assert(part and (not Surrounding[part.tag]),
          "weirdness encountered analyzing element")
   for _, mod in pairs(modifier) do
      if mod then
         elem[mod.tag] = true
         local traits = Prop[mod.tag]
         if traits then
            for trait in pairs(traits) do
               elem[trait] = true
            end
         end
      end
   end
   -- strip now-extraneous information
   for i = 1, #elem do
      elem[i] = nil
   end
   elem[1] = part
   if backrefed then
      elem[2] = modifier.backref
   end
end
#/lua


#!lua
function Mem.grammar.synthesize(grammar)
   grammar.start = grammar :take 'rule'
   synthesize(grammar)

   return grammar
end
#/lua

*** grammar:collectRules()

This builds up a large collection of relational information, while decorating
rules and names with tokens representing their normalized value.

- returns a map of the following:

  - nameSet:  The set of every name in normalized token form.

  - nameMap:  The tokens of nameSet mapped to an array of all right hand side
              references in the grammar.

  - ruleMap:  A map from the rule name (token) to the synthesized rule.

  - ruleCalls:  A map of the token for a rule to an array of the name of each
                rule it calls. This overwrites duplicate rules, which don't
                interest me very much except as something we lint and prune.

  - ruleSet:  A set containing the name of all defined rules.

  - dupe:  An array of any rule synth which has been duplicated later.  The
           tools follow the semantics of lpeg, which overwrites a =V"rule"=
           definition if it sees a second one.

  - surplus:  An array of any rule which isn't referenced by name on the
              right hand side.

  - missing:  Any rule referenced by name which isn't defined in the grammar.


#!lua
local sort, nonempty = table.sort, assert(table.nonempty)

function Mem.grammar.collectRules(grammar)
   -- our containers:
   local nameSet, nameMap = Set {}, {} -- #{token*}, token => {name*}
   local dupe, surplus, missing = {}, {}, {} -- {rule*}, {rule*}, {token*}
   local ruleMap = {}   -- token => synth
   local ruleCalls = {} -- token => {name*}
   local ruleSet = Set {}   -- #{rule_name}

   for name in grammar :filter 'name' do
      local token = normalize(name:span())
      name.token = token
      nameSet[token] = true
      local refs = nameMap[token] or {}
      insert(refs, name)
      nameMap[token] = refs
   end

   local start_rule = grammar :take 'rule'

   for rule in grammar :filter 'rule' do
      local token = assert(rule.token)
      rule.references = nameMap[token]
      ruleSet[token] = true
      if ruleMap[token] then
         -- lpeg uses the *last* rule defined so we do likewise
         ruleMap[token].duplicate = true
         insert(dupe, ruleMap[token])
      end
      ruleMap[token] = rule
      if not nameSet[token] then
         --  While it is valid to refer to the top rule, it isn't noteworthy
         --  when a grammar does not.
         --  Rules which are not findable from the start rule aren't part of
         --  the grammar, and are therefore surplus
         if rule ~= start_rule then
            rule.surplus = true
            insert(surplus, rule)
         end
      end
      -- build call graph
      local calls = {}
      ruleCalls[token] = calls
      for name in rule :filter 'name' do
         local tok = normalize(name:span())
         insert(calls, tok)
      end
   end

   -- account for missing rules (referenced but not defined)
   for name in pairs(nameSet) do
      if not ruleMap[name] then
         insert(missing, name)
      end
   end
   sort(missing)

   return { nameSet   =  nameSet,
            nameMap   =  nameMap,
            ruleMap   =  ruleMap,
            ruleCalls =  ruleCalls,
            ruleSet   =  ruleSet,
            dupe      =  nonempty(dupe),
            surplus   =  nonempty(surplus),
            missing   =  nonempty(missing), }
end
#/lua


**** partition(ruleCalls)

This partitions the rules into regular and recursive.

'Regular' here is not 100% identical to 'regular language' due to references
and lookahead, but it's suitably close.

#improve Set arithmetic is clean to write and easy to reason about, but using
it as an accumulator i.e. ==set = set + newSet== is generally wasteful and
we can drop some allocation pressure by iteration and setting to true.  This
would call for profiling, and is only worth considering because programmatic
generation of fairly complex grammar is on the horizon.

#!lua
local function partition(ruleCalls, callSet)
   local base_rules = Set()
   for name, calls in pairs(ruleCalls) do
      if #calls == 0 then
         base_rules[name] = true
         callSet[name] = nil
      end
   end

   local rule_order = {base_rules}
   local all_rules, next_rules = base_rules, Set()
   local TRIP_AT = 512
   local relaxing, trip = true, 1
   while relaxing do
      trip = trip + 1
      for name, calls in pairs(callSet) do
         local based = true
         for call in pairs(calls) do
            if not all_rules[call] then
               based = false
            end
         end
         if based then
            next_rules[name] = true
            callSet[name] = nil
         end
      end
      if #next_rules == 0 then
         relaxing = false
      else
         insert(rule_order, next_rules)
         all_rules = all_rules + next_rules
         next_rules = Set()
      end

      if trip > TRIP_AT then
         relaxing = false
         error "512 attempts to relax rule order, something is off"
      end
   end

   return rule_order, callSet
end
#/lua

*** Mem.grammar.callSet(grammar)

This makes Sets non-destructively out of arrays of rule names, which might not
have to be non-destructive, but comes with no disadvantages at this point.

#!lua
local clone1 = assert(table.clone1)

local function _callSet(ruleCalls)
   local callSet = {}
   for name, calls in pairs(ruleCalls) do
      callSet[name] = Set(clone1(calls))
   end
   return callSet
end
#/lua

#!lua
function Mem.grammar.callSet(grammar)
   local collection = grammar.collection or grammar:collectRules()
   return _callSet(collection.ruleCalls)
end
#/lua


**** graphCalls(grammar)

  Now that we've obtained all the terminal rules, we can use more set
addition and a queue to obtain the full rule set seen by any other given
rule.

This returns a map of names to a Set of every rule which can be visited from
that rule name, followed by the regular and recursive halves, which we do not
currently collect.

#!lua
local function setFor(tab)
   return Set(clone1(tab))
end

local function graphCalls(grammar)
   local collection = assert(grammar.collection)
   local ruleCalls, ruleMap = assert(collection.ruleCalls),
                               assert(collection.ruleMap)
   local regulars = assert(collection.regulars)

   -- go through each layer and build the full dependency tree for regular
   -- rules
   local regSets = {}

   -- first set of rules have no named subrules
   -- which we call 'final'
   local depSet = regulars[1]
   for name in pairs(depSet) do
      ---[[DBG]] ruleMap[name].final = true
      regSets[name] = Set {}
   end
   -- second tier has only the already-summoned direct calls
   depSet = regulars[2] or {}
   for name in pairs(depSet) do
      regSets[name] = setFor(ruleCalls[name])
   end
   -- the rest is set arithmetic
   for i = 3, #regulars do
      depSet = regulars[i]
      for name in pairs(depSet) do
         local callSet = setFor(ruleCalls[name])
         for _, called in ipairs(ruleCalls[name]) do
            callSet = callSet + regSets[called]
         end
         regSets[name] = callSet
      end
   end

   --  the regulars collected, we turn to the recursives and roll 'em up
   local recursive = assert(collection.recursive)
   local recurSets = {}

   -- make a full recurrence graph for one set
   local function oneGraph(name, callSet)
      local recurSet = callSet + {}
      -- start with known subsets
      for elem in pairs(callSet) do
         local subSet = regSets[elem] or recurSets[elem]
         if subSet then
            recurSet = recurSet + subSet
         end
      end
      -- run a queue until we're out of names
      local shuttle = Deque()
      for elem in pairs(recurSet) do
         shuttle:push(elem)
      end
      for elem in shuttle:popAll() do
         for _, name in ipairs(ruleCalls[elem] or {}) do
            if not recurSet[name] then
               shuttle:push(name)
               recurSet[name] = true
            end
         end
      end

      recurSets[name] = recurSet
   end

   for name, callSet in pairs(recursive) do
      oneGraph(name, callSet)
   end
   local allCalls = clone1(regSets)
   for name, set in pairs(recurSets) do
      allCalls[name] = set
   end
   return allCalls, regSets, recurSets
end
#/lua


**** trimRecursive(recursive)

#!lua
local function trimRecursive(recursive, ruleMap)
   for rule, callset in pairs(recursive) do
      for elem in pairs(callset) do
         if (not ruleMap[elem])
            or (not ruleMap[elem].recursive) then
            callset[elem] = nil
         end
      end
   end

   return recursive
end
#/lua


*** grammar:analyze()

Pulls together the caller-callee relationships.

#!lua
function Mem.grammar.analyze(grammar)
   grammar.collection = grammar:collectRules()
   local coll = assert(grammar.collection)

   local regulars, recursive = partition(coll.ruleCalls, grammar:callSet())
   local ruleMap = assert(coll.ruleMap)
   for name in pairs(recursive) do
      ruleMap[name].recursive = true
   end
   coll.regulars, coll.recursive = regulars, trimRecursive(recursive, ruleMap)
   coll.calls = graphCalls(grammar)
   if coll.missing then
      grammar:makeDummies()
   end

   -- we'll switch to using these directly
   for k, v in pairs(coll) do
      grammar[k] = v
   end


   return grammar:anomalies()
end
#/lua


** Anomalous Grammars

A grammar is anomalous if it has missing, duplicate, or surplus rules.

Duplicate rules are a simple error, since the only semantic of a duplicate
rule is to overwrite the earlier with the latter, so we have no further
action to take in the moments before the mistake is corrected by the user.

Missing rules we can account for by building placeholders, which we do.

Surplus rules are an interesting case, because there is a coherent kind of
surplus rule or rules: an alternate grammar built partially or wholly from
rules referenced from the start rule of the grammar.

The intention of the Vav framework is that it will be tractable to assemble
this sort of rule from parts, and this is more clear than embedding several
grammars into one.

But it's a coherent action to take on surplus rules, the semantic is clear
enough, and we'll consider doing it.


*** grammar:anomalies()

If everything is in order, returns =nil, message=, otherwise, the
less-than-perfect aspects of the grammar as-is.

#!lua
function Mem.grammar.anomalies(grammar)
   local coll = grammar.collection
   if not coll then return nil, "collectRules first" end
   if not (grammar.missing or grammar.surplus or grammar.dupe) then
      return nil, "no anomalies detected"
   else
      return { missing = grammar.missing,
               surplus = grammar.surplus,
               dupe   = grammar.dupe }
   end
end
#/lua



*** Codegen Mixin

This should be a Clade vector.

#!lua
local codegen = require "espalier:peg/codegen"

for class, mixin in pairs(codegen) do
   for trait, method in pairs(mixin) do
      Mem[class][trait] = method
   end
end
#/lua


#!lua
return MemClade
#/lua

